{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4-HzXrhlU37"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperParameters**"
      ],
      "metadata": {
        "id": "HpqBKoeHqWbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 128\n",
        "max_len = 9\n",
        "\n",
        "embedding_dim = 100 # word embedding\n",
        "# GRU\n",
        "num_layers = 1\n",
        "hidden_size = 64"
      ],
      "metadata": {
        "id": "80McYKdYn7JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Num_Sequence**"
      ],
      "metadata": {
        "id": "t5szCX6Bqfap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Num_sequence:\n",
        "  UNK_TAG = 'UNK'\n",
        "  PAD_TAG = 'PAD'\n",
        "  SOS_TAG = 'SOS' # start of sequence\n",
        "  EOS_TAG = 'EOS' # end of sequence\n",
        "\n",
        "  UNK = 0\n",
        "  PAD = 1\n",
        "  SOS = 2\n",
        "  EOS = 3\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.dict = {\n",
        "        self.PAD_TAG : self.PAD,\n",
        "        self.UNK_TAG : self.UNK,\n",
        "        self.SOS_TAG : self.SOS,\n",
        "        self.EOS_TAG : self.EOS,\n",
        "         }\n",
        "\n",
        "    for i in range(10):\n",
        "      self.dict[str(i)] = len(self.dict)\n",
        "    \n",
        "    self.inverse_dict = dict(zip(self.dict.values(), self.dict.keys()))\n",
        "  \n",
        "  def transform(self, sentence, max_len, add_eos=False):\n",
        "    '''sentence 2 number\n",
        "    add_eos: True: sentence length = max_len + 1\n",
        "    add_eos: False: sentence length = max_len\n",
        "    \n",
        "    '''\n",
        "    if len(sentence) > max_len: # cut if sentence > max_len\n",
        "      sentence = sentence[:max_len]\n",
        "\n",
        "    sentence_len = len(sentence)  # must calculate lenth of sentence previously\n",
        "      \n",
        "    if add_eos:\n",
        "      sentence = sentence + [self.EOS_TAG]\n",
        "\n",
        "    if sentence_len < max_len: # add padding if sentence < max_len\n",
        "      sentence = sentence + [self.PAD_TAG]*(max_len-sentence_len)\n",
        "\n",
        "\n",
        "    result = [self.dict.get(i, self.UNK) for i in sentence]\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def inverse_transform(self, indices):\n",
        "    '''seq 2 sentence'''\n",
        "    [self.inverse_dict.get(i, self.UNK_TAG) for i in indices]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dict)"
      ],
      "metadata": {
        "id": "EhZBqo7wqiQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequence = Num_sequence()"
      ],
      "metadata": {
        "id": "qu9UyKX0urPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**  \n",
        "Prepare dataset and dataloader"
      ],
      "metadata": {
        "id": "EkMDv27gldpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In targets of the samples, EOS and SOS are needed to label the start and the end of the network.  \n",
        "2. Add EOS in the target and transform.  "
      ],
      "metadata": {
        "id": "tl2BZgmCtfwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NumDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # generate random number with numpy\n",
        "    self.data = np.random.randint(0, 1e8, size=[500000])\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    input = list(str(self.data[index]))\n",
        "    label = input + ['0']\n",
        "    input_length = len(input)\n",
        "    label_length = len(label)\n",
        "    return input, label, input_length, label_length\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "0IdOMtxrlfum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  '''\n",
        "  :param batch: [(input, label, input_length, label_length), (input, label, input_length, label_length)]\n",
        "  :return:\n",
        "  '''\n",
        "\n",
        "  batch = sorted(batch, key=lambda x: x[3], reverse=True) # big -> small\n",
        "  \n",
        "  input, target, input_length, target_length = zip(*batch)\n",
        "\n",
        "  input = [num_sequence.transform(i, max_len=max_len) for i in input]\n",
        "  target = [num_sequence.transform(i, max_len=max_len+1) for i in target]\n",
        "\n",
        "  input = torch.LongTensor(input)\n",
        "  target = torch.LongTensor(target)\n",
        "  input_length = torch.LongTensor(input_length)\n",
        "  target_length = torch.LongTensor(target_length)\n",
        "\n",
        "\n",
        "  return input, target, input_length, target_length"
      ],
      "metadata": {
        "id": "i7aJ2Kdyoa5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = NumDataset()\n",
        "train_data_loader = DataLoader(data_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ZiH0_HS4ns18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "t3o-xnVzlpZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using GRU, there are two API for accelerating the calculation.  \n",
        "1. pad_packed_sequence(out, batct_first, padding_value) *unpack*\n",
        "2. pack_padded_sequence(embedded, real_length, batch_first) *pack*\n",
        "3. Before using the two API, sort the batch in descending order."
      ],
      "metadata": {
        "id": "rlfiN7TQs5__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
      ],
      "metadata": {
        "id": "u-l8Wub8nFUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(num_sequence), embedding_dim=embedding_dim, padding_idx=num_sequence.PAD)\n",
        "    self.gru = nn.GRU(input_size=embedding_dim, num_layers=num_layers, hidden_size=hidden_size, batch_first=True)\n",
        "  \n",
        "  def forward(self, input, input_length):\n",
        "    '''\n",
        "    :param input: [batch_size, max_len]\n",
        "    :return \n",
        "    '''\n",
        "    embeded = self.embedding(input) # [batch_size, max_len, embedding_dim]\n",
        "\n",
        "    # pack to accelerate calculation\n",
        "    embeded = pack_padded_sequence(embeded, input_length, batch_first=True)\n",
        "\n",
        "    output, hidden = self.gru(embeded)\n",
        "\n",
        "    # unpack\n",
        "    output, output_length = pad_packed_sequence(output, batch_first=True)\n",
        "\n",
        "    # hidden: [1*1, batch_size, hidden_size]\n",
        "    # output: [batch_size, seq_len, hidden_size]\n",
        "    return output, hidden, output_length\n"
      ],
      "metadata": {
        "id": "YGWolPMclqo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "print(encoder)\n",
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  out, hidden, output_length = encoder(input, input_length)\n",
        "  print(out.size())\n",
        "  print(hidden.size())\n",
        "  print(output_length)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSrN2eXfu0u0",
        "outputId": "11f5a995-59cb-473c-f007-fba45aa7bc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            ")\n",
            "torch.Size([128, 8, 64])\n",
            "torch.Size([1, 128, 64])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "7wj9udQOnMWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The out put of the encoder is a classification problem. We choose the output with a highest probability. \n",
        "2. The output of the decoder is [batch_size, max_len, vocab_size].\n",
        "3. Loss function: Cross Entropy"
      ],
      "metadata": {
        "id": "0zR0yKFooXS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JNqldpOu1q8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(num_sequence), embedding_dim=embedding_dim,padding_idx=num_sequence.PAD)\n",
        "    self.gru = nn.GRU(input_size=embedding_dim,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=num_layers,\n",
        "                      batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, len(num_sequence))\n",
        "  \n",
        "  def forward(self, target, encoder_hidden):\n",
        "    # 1. Get output from encoder, pass it into the hidden_state of decoder for the fitst time\n",
        "    decoder_hidden = encoder_hidden\n",
        "    # 2. Prepare the input for decoder for the first time, SOS with size of [batch_size, 1]\n",
        "    batch_size = target.size(0)\n",
        "    decoder_input = torch.LongTensor(torch.ones([batch_size, 1], dtype=torch.int64))*num_sequence.SOS\n",
        "    # 3. Calculate at the first time stamp, get output and hidden_state\n",
        "\n",
        "    # 4. Calculate the next output according to previous output\n",
        "    # 5. Put previous hidden_state and output as current hidden_state and input\n",
        "    # 6. Recurrsion step 4 and step 5\n",
        "    for i in range(max_len + 2):\n",
        "      decoder_output_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "\n",
        "      value, index = torch.topk(decoder_output_t, 1)\n",
        "      decoder_input = index\n",
        "  \n",
        "  def forward_step(self, decoder_input, decoder_hidden):\n",
        "    '''\n",
        "    calculate output at each time stamp\n",
        "    :param decoder_input: [batch_size, 1]\n",
        "    :param decoder_hidden: [1, batch_size, hidden_size]\n",
        "    :return:    \n",
        "    '''\n",
        "    decoder_input_embedded = self.embedding(decoder_input)  # [batch_size, 1, embedding_dim]\n",
        "\n",
        "    # out: [batch_size, 1, hidden_size] It is 1 because at the first point seq_len=1\n",
        "    # decoder_hidden: [1, batch_size, hidden_size]\n",
        "    out, decoder_hidden = self.gru(decoder_input_embedded)\n",
        "\n",
        "    out = out.squeeze(1) # [batch_size, hidden_size]\n",
        "    out = self.fc(out)  # [batch_size, vocab_size]\n",
        "    output = F.log_softmax(out, dim=-1) # [batch_size, vocab_size]\n",
        "\n",
        "    return output, decoder_hidden\n",
        "    "
      ],
      "metadata": {
        "id": "tCV0XPxKnZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  out, encoder_hidden, _ = encoder(input, input_length)\n",
        "  decoder(target, encoder_hidden)\n",
        "  print(out.size())\n",
        "  print(hidden.size())\n",
        "  print(output_length)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X77HPtfrSGxc",
        "outputId": "92152dca-c405-41d3-cb17-b6c13a19047b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=14, bias=True)\n",
            ")\n",
            "torch.Size([128, 8, 64])\n",
            "torch.Size([1, 128, 64])\n",
            "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
            "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7])\n"
          ]
        }
      ]
    }
  ]
}