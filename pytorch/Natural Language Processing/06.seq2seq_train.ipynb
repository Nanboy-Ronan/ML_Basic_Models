{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4-HzXrhlU37"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperParameters**"
      ],
      "metadata": {
        "id": "HpqBKoeHqWbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 256\n",
        "max_len = 10\n",
        "\n",
        "embedding_dim = 100 # word embedding\n",
        "# GRU\n",
        "num_layers = 1\n",
        "hidden_size = 64\n",
        "\n",
        "# Train\n",
        "epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Test\n",
        "test_batch_size = 1000"
      ],
      "metadata": {
        "id": "80McYKdYn7JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Num_Sequence**"
      ],
      "metadata": {
        "id": "t5szCX6Bqfap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Num_sequence:\n",
        "  UNK_TAG = '<UNK>'\n",
        "  PAD_TAG = '<PAD>'\n",
        "  SOS_TAG = '<SOS>' # start of sequence\n",
        "  EOS_TAG = '<EOS>' # end of sequence\n",
        "\n",
        "  UNK = 0\n",
        "  PAD = 1\n",
        "  SOS = 2\n",
        "  EOS = 3\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.dict = {\n",
        "        self.PAD_TAG : self.PAD,\n",
        "        self.UNK_TAG : self.UNK,\n",
        "        self.SOS_TAG : self.SOS,\n",
        "        self.EOS_TAG : self.EOS,\n",
        "         }\n",
        "\n",
        "    for i in range(10):\n",
        "      self.dict[str(i)] = len(self.dict)\n",
        "    \n",
        "    self.inverse_dict = dict(zip(self.dict.values(), self.dict.keys()))\n",
        "  \n",
        "  def transform(self, sentence, max_len=None, add_eos=False):\n",
        "    '''string 2 vector\n",
        "    :param sentence:  str or list(), \"123...\" or [\"1\",\"2\",\"5\" ... str]\n",
        "    :param: max_len: int\n",
        "    add_eos: if to add \"\"<EOS> True: sentence length = max_len + 1\n",
        "                               False: sentence length = max_len\n",
        "    :return: [int, int, int ...]\n",
        "    '''\n",
        "\n",
        "    if add_eos:\n",
        "      assert(max_len != None)\n",
        "      max_len = max_len - 1\n",
        "\n",
        "    if max_len is not None:\n",
        "      if len(sentence) > max_len: # cut if sentence > max_len\n",
        "        sentence = sentence[:max_len]\n",
        "      else: # add padding if sentence < max_len\n",
        "        sentence = sentence + [self.PAD_TAG]*(max_len-len(sentence))\n",
        "      \n",
        "    if add_eos:\n",
        "      if sentence[-1] == self.PAD_TAG:  # if there is PAD in the sentence, add EOS before the TAG\n",
        "        pad_index = sentence.index(self.PAD_TAG)\n",
        "        sentence.insert(pad_index, self.EOS_TAG)\n",
        "      else: # No pad, and EOS at the end of the sentence\n",
        "        sentence.append(self.EOS_TAG)\n",
        "\n",
        "    result = [self.dict.get(i, self.UNK) for i in sentence]\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def inverse_transform(self, indices):\n",
        "    '''vector 2 string\n",
        "    :param indices: [int, int, int, ...]\n",
        "    :return: \"123123...\"    \n",
        "    '''\n",
        "    result = []\n",
        "    for i in indices:\n",
        "      temp = self.inverse_dict.get(i, self.UNK_TAG)\n",
        "      if temp != self.EOS_TAG:  # delete everything after EOS\n",
        "        result.append(temp)\n",
        "      else:\n",
        "        break\n",
        "    return \"\".join(result)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dict)"
      ],
      "metadata": {
        "id": "EhZBqo7wqiQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequence = Num_sequence()"
      ],
      "metadata": {
        "id": "qu9UyKX0urPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test num_Sequence\n",
        "print(num_sequence.dict)\n",
        "s = \"123123\"\n",
        "retVal = num_sequence.transform(s)\n",
        "print(retVal)\n",
        "retVal = num_sequence.inverse_transform(retVal)\n",
        "print(retVal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU8OTYTx4k-Z",
        "outputId": "7010f15c-3fa3-4b9b-c754-6f527da7a293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 1, '<UNK>': 0, '<SOS>': 2, '<EOS>': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13}\n",
            "[5, 6, 7, 5, 6, 7]\n",
            "123123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**  \n",
        "Prepare dataset and dataloader"
      ],
      "metadata": {
        "id": "EkMDv27gldpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In targets of the samples, EOS and SOS are needed to label the start and the end of the network.  \n",
        "2. Add EOS in the target and transform.  "
      ],
      "metadata": {
        "id": "tl2BZgmCtfwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NumDataset(Dataset):\n",
        "  def __init__(self, train=True):\n",
        "    # generate random number with numpy\n",
        "    np.random.seed(10) if train else np.random.seed(11)\n",
        "    self.size = 400000 if train else 100000\n",
        "    self.data = np.random.randint(0, 1e8, size=[self.size])\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    input = list(str(self.data[index]))\n",
        "    target = input + ['0']\n",
        "    input_length = len(input)\n",
        "    target_length = len(target)\n",
        "    return input, target, input_length, target_length\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "0IdOMtxrlfum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  '''\n",
        "  :param batch: [(input, label, input_length, label_length), (input, label, input_length, label_length)]\n",
        "  :return:\n",
        "  '''\n",
        "\n",
        "  batch = sorted(batch, key=lambda x: x[2], reverse=True) # big -> small\n",
        "  \n",
        "  input, target, input_length, target_length = zip(*batch)\n",
        "\n",
        "  input = [num_sequence.transform(i, max_len=max_len) for i in input]\n",
        "  target = [num_sequence.transform(i, max_len=max_len, add_eos=True) for i in target]\n",
        "  input = torch.LongTensor(input)\n",
        "  target = torch.LongTensor(target)\n",
        "\n",
        "  input_length = torch.LongTensor(input_length)\n",
        "  target_length = torch.LongTensor(target_length)\n",
        "\n",
        "\n",
        "  return input, target, input_length, target_length"
      ],
      "metadata": {
        "id": "i7aJ2Kdyoa5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = NumDataset(train=True)\n",
        "train_data_loader = DataLoader(data_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ZiH0_HS4ns18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  print(input.size())\n",
        "  print(target.size())\n",
        "  print(input_length.shape)\n",
        "  print(target_length.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDgleUltC9nu",
        "outputId": "4781a2eb-06d0-414a-aceb-d767da1c8bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = list(\"abcd\")\n",
        "temp += [\"ADD\"]\n",
        "print(temp)\n",
        "\n",
        "temp = list(\"abcd\")\n",
        "temp.append(\"ADD\")\n",
        "print(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uVqzRDDGCYX",
        "outputId": "34077525-6ffc-4fef-b4c5-0f49a7c83542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'ADD']\n",
            "['a', 'b', 'c', 'd', 'ADD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "t3o-xnVzlpZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using GRU, there are two API for accelerating the calculation.  \n",
        "1. pad_packed_sequence(out, batct_first, padding_value) *unpack*\n",
        "2. pack_padded_sequence(embedded, real_length, batch_first) *pack*\n",
        "3. Before using the two API, sort the batch in descending order."
      ],
      "metadata": {
        "id": "rlfiN7TQs5__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
      ],
      "metadata": {
        "id": "u-l8Wub8nFUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(num_sequence), embedding_dim=embedding_dim, padding_idx=num_sequence.PAD)\n",
        "    self.gru = nn.GRU(input_size=embedding_dim, num_layers=num_layers, hidden_size=hidden_size, batch_first=True, bidirectional=False, dropout=0)\n",
        "  \n",
        "  def forward(self, input, input_length):\n",
        "    '''\n",
        "    :param input: [batch_size, max_len]\n",
        "    :return \n",
        "    '''\n",
        "    embeded = self.embedding(input) # [batch_size, max_len, embedding_dim]\n",
        "\n",
        "    # pack to accelerate calculation\n",
        "    embeded = pack_padded_sequence(embeded, input_length.cpu(), batch_first=True)\n",
        "\n",
        "    output, hidden = self.gru(embeded)\n",
        "\n",
        "    # unpack\n",
        "    output, output_length = pad_packed_sequence(output, batch_first=True, padding_value=num_sequence.PAD)\n",
        "\n",
        "    # hidden: [1*1, batch_size, hidden_size]\n",
        "    # output: [batch_size, seq_len, hidden_size]\n",
        "    return output, hidden\n"
      ],
      "metadata": {
        "id": "YGWolPMclqo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "print(encoder)\n",
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  out, hidden = encoder(input, input_length)\n",
        "  print(out.size())\n",
        "  print(hidden.size())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSrN2eXfu0u0",
        "outputId": "738345bd-21d0-40d7-a890-4dd39096f536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            ")\n",
            "torch.Size([256, 8, 64])\n",
            "torch.Size([1, 256, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "7wj9udQOnMWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The output of the encoder is a classification problem. We choose the output with a highest probability. \n",
        "2. The output of the decoder is [batch_size, max_len, vocab_size].\n",
        "3. Loss function: Cross Entropy"
      ],
      "metadata": {
        "id": "0zR0yKFooXS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JNqldpOu1q8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(num_sequence), embedding_dim=embedding_dim,padding_idx=num_sequence.PAD)\n",
        "    # hidden_state = [1, batch_size, hidden_size]\n",
        "    self.gru = nn.GRU(input_size=embedding_dim,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=num_layers,\n",
        "                      batch_first=True,\n",
        "                      bidirectional=False,\n",
        "                      dropout=0)\n",
        "    # encoder_hidden_t: [2, batch_size, hidden_size]\n",
        "    self.fc = nn.Linear(hidden_size, len(num_sequence))\n",
        "  \n",
        "  def forward(self, target, encoder_hidden):\n",
        "    # 1. For the first time, decoder has the same hidden state as the encoder\n",
        "    decoder_hidden = encoder_hidden # [1, batch_size, hidden_size]\n",
        "    # 2. For the first time, input of decoder is the SOS with size of [batch_size, 1]\n",
        "    batch_size = encoder_hidden.size(1)\n",
        "    # decoder_input = torch.LongTensor(torch.ones([batch_size, 1], dtype=torch.int64)*num_sequence.SOS).to(device)\n",
        "    decoder_input = torch.LongTensor([[num_sequence.SOS]]*batch_size).to(device)\n",
        "    # 3. Calculate at the first time stamp, get output and hidden_state\n",
        "\n",
        "    # 4. Calculate the next output according to previous output\n",
        "    # 5. Put previous hidden_state and output as current hidden_state and input\n",
        "    # 6. Recurrsion step 4 and step 5\n",
        "\n",
        "    # Save the result of prediction\n",
        "    # [batch_size, max_len, vocab_size]\n",
        "    decoder_outputs = torch.zeros([batch_size, max_len, len(num_sequence)]).to(device)\n",
        "\n",
        "    for t in range(max_len):\n",
        "      decoder_output_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "      # decoder_output_t: [batch_size, vocab_size]; decoder_hidden: [1, batch_size, hidden_size]\n",
        "      # save decoder_output_t\n",
        "      decoder_outputs[:,t,:] = decoder_output_t\n",
        "\n",
        "      value, index = decoder_output_t.max(dim=-1)\n",
        "      decoder_input = index.unsqueeze(-1)\n",
        "    \n",
        "    return decoder_outputs, decoder_hidden\n",
        "  \n",
        "  def forward_step(self, decoder_input, decoder_hidden):\n",
        "    '''\n",
        "    calculate output at each time stamp\n",
        "    :param decoder_input: [batch_size, 1]\n",
        "    :param decoder_hidden: [1, batch_size, hidden_size]\n",
        "    :return:    \n",
        "    '''\n",
        "    decoder_input_embedded = self.embedding(decoder_input)  # [batch_size, 1, embedding_dim]\n",
        "\n",
        "    # out: [batch_size, 1, hidden_size] It is 1 because at the first point seq_len=1\n",
        "    # decoder_hidden: [1, batch_size, hidden_size]\n",
        "    out, decoder_hidden = self.gru(decoder_input_embedded, decoder_hidden)\n",
        "    # out: [batch_size, 1, hidden_size]\n",
        "    out = out.squeeze(dim=1) # [batch_size, hidden_size]\n",
        "    out = self.fc(out)  # [batch_size, vocab_size]\n",
        "    output = F.log_softmax(out, dim=-1) # [batch_size, vocab_size]\n",
        "\n",
        "    return output, decoder_hidden\n",
        "\n",
        "  \n",
        "  def evaluate(self, encoder_hidden):\n",
        "    decoder_hidden = encoder_hidden\n",
        "    batch_size = encoder_hidden.size(1)\n",
        "    decoder_output = torch.LongTensor(torch.ones([]))\n",
        "    decoder_input = torch.LongTensor(torch.ones([batch_size, 1], dtype=torch.int64)*num_sequence.SOS).to(device)\n",
        "\n",
        "    indices = []\n",
        "    for i in range(max_len+5):\n",
        "      decoder_output_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "      value, index = torch.topk(decoder_output_t, 1) # [batch_size, 1]\n",
        "      decoder_input = index\n",
        "      indices.append(index.squeeze(-1))\n",
        "      \n",
        "    return indices\n",
        "    "
      ],
      "metadata": {
        "id": "tCV0XPxKnZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  out, encoder_hidden = encoder(input, input_length)\n",
        "  decoder(target, encoder_hidden)\n",
        "  print(out.size())\n",
        "  print(hidden.size())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X77HPtfrSGxc",
        "outputId": "ccb47d99-3fd7-4e74-a05e-16e4fee1ea5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=14, bias=True)\n",
            ")\n",
            "torch.Size([256, 8, 64])\n",
            "torch.Size([1, 256, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine encoder and decoder to get seq2seq"
      ],
      "metadata": {
        "id": "wEJkYFM93lpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder()\n",
        "  \n",
        "  def forward(self, input, target, input_length, target_length):\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input, input_length)\n",
        "    decoder_outputs, decoder_hidden = self.decoder(target, encoder_hidden)\n",
        "    # print('encoder_outputs', encoder_outputs.shape)\n",
        "    # print('decoder_outputs', decoder_outputs.shape)\n",
        "    # print('target', target.size())\n",
        "    return decoder_outputs, decoder_hidden\n",
        "  \n",
        "  def evaluate(self, input, input_length):\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input, input_length)\n",
        "    indices = self.decoder.evaluate(encoder_hidden)\n",
        "    return indices\n",
        "    "
      ],
      "metadata": {
        "id": "j0Dd4aPL3roX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize the model, optimizer and loss.  \n",
        "2. Traverse dataloader.\n",
        "3. Produce output from the model\n",
        "4. Calculate the loss\n",
        "5. Save and Load the model\n"
      ],
      "metadata": {
        "id": "JiVjaWUr4ttc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "HGkXuff-5HUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq().to(device)\n",
        "optimizer = Adam(seq2seq.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "TjIOXYkC5B5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "SZrzzIy54hdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "eP5tdOa5ICLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory\n",
        "if not os.path.exists(\"./models\"):\n",
        "  os.mkdir(\"./models\")"
      ],
      "metadata": {
        "id": "zmQfOErdIPiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for index, (input, target, input_length, target_length) in enumerate(train_data_loader):\n",
        "    input, target, input_length, target_length = input.to(device), target.to(device), input_length.to(device), target_length.to(device)\n",
        "    decoder_outputs, _ = seq2seq(input, target, input_length, target_length)\n",
        "    # print(decoder_outputs.size(), target.size())\n",
        "    decoder_outputs = decoder_outputs.view(-1, len(num_sequence)) # [batch_size*seq_len, -1]\n",
        "    # print(decoder_outputs.size(), target.size())\n",
        "    target = target.view(-1)  # [batch_size*seq_len]\n",
        "    loss = F.nll_loss(decoder_outputs, target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Record the Training Phase\n",
        "    if index % 300 == 0:\n",
        "      torch.save(seq2seq.state_dict(), \"./models/model.pkl\")\n",
        "      torch.save(optimizer.state_dict(), \"./models/optimizer.pkl\")\n",
        "      pickle.dump(loss_list, open(\"./models/loss_list.pkl\", \"wb\"))\n",
        "      print(\"epoch: {}\\t idx:{} \\t loss:{: .6f}\".format(epoch, index, loss.item()))"
      ],
      "metadata": {
        "id": "4MwFa6rg3lAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be008177-2fd0-47d0-b619-7281da522f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\t idx:0 \t loss: 2.666190\n",
            "epoch: 0\t idx:300 \t loss: 1.275529\n",
            "epoch: 0\t idx:600 \t loss: 0.963875\n",
            "epoch: 0\t idx:900 \t loss: 0.739106\n",
            "epoch: 0\t idx:1200 \t loss: 0.524797\n",
            "epoch: 0\t idx:1500 \t loss: 0.394668\n",
            "epoch: 1\t idx:0 \t loss: 0.368204\n",
            "epoch: 1\t idx:300 \t loss: 0.268982\n",
            "epoch: 1\t idx:600 \t loss: 0.178646\n",
            "epoch: 1\t idx:900 \t loss: 0.133924\n",
            "epoch: 1\t idx:1200 \t loss: 0.104854\n",
            "epoch: 1\t idx:1500 \t loss: 0.083341\n",
            "epoch: 2\t idx:0 \t loss: 0.069642\n",
            "epoch: 2\t idx:300 \t loss: 0.049697\n",
            "epoch: 2\t idx:600 \t loss: 0.050612\n",
            "epoch: 2\t idx:900 \t loss: 0.043680\n",
            "epoch: 2\t idx:1200 \t loss: 0.027638\n",
            "epoch: 2\t idx:1500 \t loss: 0.030529\n",
            "epoch: 3\t idx:0 \t loss: 0.022340\n",
            "epoch: 3\t idx:300 \t loss: 0.030367\n",
            "epoch: 3\t idx:600 \t loss: 0.012406\n",
            "epoch: 3\t idx:900 \t loss: 0.029411\n",
            "epoch: 3\t idx:1200 \t loss: 0.019299\n",
            "epoch: 3\t idx:1500 \t loss: 0.012275\n",
            "epoch: 4\t idx:0 \t loss: 0.016924\n",
            "epoch: 4\t idx:300 \t loss: 0.011942\n",
            "epoch: 4\t idx:600 \t loss: 0.037835\n",
            "epoch: 4\t idx:900 \t loss: 0.006790\n",
            "epoch: 4\t idx:1200 \t loss: 0.004283\n",
            "epoch: 4\t idx:1500 \t loss: 0.007866\n",
            "epoch: 5\t idx:0 \t loss: 0.009846\n",
            "epoch: 5\t idx:300 \t loss: 0.014399\n",
            "epoch: 5\t idx:600 \t loss: 0.007919\n",
            "epoch: 5\t idx:900 \t loss: 0.005468\n",
            "epoch: 5\t idx:1200 \t loss: 0.009047\n",
            "epoch: 5\t idx:1500 \t loss: 0.003187\n",
            "epoch: 6\t idx:0 \t loss: 0.007076\n",
            "epoch: 6\t idx:300 \t loss: 0.005177\n",
            "epoch: 6\t idx:600 \t loss: 0.005272\n",
            "epoch: 6\t idx:900 \t loss: 0.022948\n",
            "epoch: 6\t idx:1200 \t loss: 0.002894\n",
            "epoch: 6\t idx:1500 \t loss: 0.003998\n",
            "epoch: 7\t idx:0 \t loss: 0.006208\n",
            "epoch: 7\t idx:300 \t loss: 0.028424\n",
            "epoch: 7\t idx:600 \t loss: 0.005089\n",
            "epoch: 7\t idx:900 \t loss: 0.002441\n",
            "epoch: 7\t idx:1200 \t loss: 0.004701\n",
            "epoch: 7\t idx:1500 \t loss: 0.001518\n",
            "epoch: 8\t idx:0 \t loss: 0.002118\n",
            "epoch: 8\t idx:300 \t loss: 0.001946\n",
            "epoch: 8\t idx:600 \t loss: 0.002990\n",
            "epoch: 8\t idx:900 \t loss: 0.003169\n",
            "epoch: 8\t idx:1200 \t loss: 0.004555\n",
            "epoch: 8\t idx:1500 \t loss: 0.025416\n",
            "epoch: 9\t idx:0 \t loss: 0.023462\n",
            "epoch: 9\t idx:300 \t loss: 0.009806\n",
            "epoch: 9\t idx:600 \t loss: 0.001386\n",
            "epoch: 9\t idx:900 \t loss: 0.016006\n",
            "epoch: 9\t idx:1200 \t loss: 0.001631\n",
            "epoch: 9\t idx:1500 \t loss: 0.002298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "lLoIt0esvSoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "data = [list(str(i)) for i in np.random.randint(0, 1e8, size=[100])]  # input\n",
        "data = sorted(data, key=lambda x:len(x), reverse=True)\n",
        "data_length = [len(i) for i in data]  # input_length"
      ],
      "metadata": {
        "id": "OfGNYt1lsolh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.LongTensor([num_sequence.transform(i, max_len) for i in data])"
      ],
      "metadata": {
        "id": "2NRIAqW1wI3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Prediction\n",
        "indices = seq2seq.evaluate(input=input, input_length=input_length)\n",
        "indices = np.array(indices).transpose()"
      ],
      "metadata": {
        "id": "Ac6MHGh6wr9A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "f5d49c5f-4f0b-433e-cfb2-107dd9532398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-f6ff24396b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-875c29f7d4f3>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input, input_length)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-de0b83412927>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, input_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# pack to accelerate calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0membeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_packed_sequence_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected `len(lengths)` to be equal to batch_size, but got 128 (batch_size=100)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse Transform\n",
        "result = []\n",
        "for line in indices:\n",
        "  temp_result = num_sequence.inverse_transform(indices)\n",
        "  cur_line = \"\"\n",
        "  for word in temp_result:\n",
        "    if word == num_sequence.EOS_TAG:\n",
        "      break\n",
        "    cur_line += word\n",
        "  result.append(cur_line)"
      ],
      "metadata": {
        "id": "j9lE1rRqxBef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Cm959FTwznYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}