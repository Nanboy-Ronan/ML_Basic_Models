{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a seq2seq model which accept an input of number (e.g. \"12345\") and will add a 0 at the end of the number (e.g. \"123450\")."
      ],
      "metadata": {
        "id": "L4G8hk9tjO7a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4-HzXrhlU37"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HyperParameters**"
      ],
      "metadata": {
        "id": "HpqBKoeHqWbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 256\n",
        "max_len = 10\n",
        "\n",
        "embedding_dim = 100 # word embedding\n",
        "# GRU\n",
        "num_layers = 1\n",
        "hidden_size = 64\n",
        "\n",
        "# Train\n",
        "epochs = 5\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Test\n",
        "test_batch_size = 1000"
      ],
      "metadata": {
        "id": "80McYKdYn7JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Num_Sequence**"
      ],
      "metadata": {
        "id": "t5szCX6Bqfap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Num_sequence:\n",
        "  UNK_TAG = '<UNK>'\n",
        "  PAD_TAG = '<PAD>'\n",
        "  SOS_TAG = '<SOS>' # start of sequence\n",
        "  EOS_TAG = '<EOS>' # end of sequence\n",
        "\n",
        "  UNK = 0\n",
        "  PAD = 1\n",
        "  SOS = 2\n",
        "  EOS = 3\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.dict = {\n",
        "        self.PAD_TAG : self.PAD,\n",
        "        self.UNK_TAG : self.UNK,\n",
        "        self.SOS_TAG : self.SOS,\n",
        "        self.EOS_TAG : self.EOS,\n",
        "         }\n",
        "\n",
        "    for i in range(10):\n",
        "      self.dict[str(i)] = len(self.dict)\n",
        "    \n",
        "    self.inverse_dict = dict(zip(self.dict.values(), self.dict.keys()))\n",
        "  \n",
        "  def transform(self, sentence, max_len=None, add_eos=False):\n",
        "    '''string 2 vector\n",
        "    :param sentence:  str or list(), \"123...\" or [\"1\",\"2\",\"5\" ... str]\n",
        "    :param: max_len: int\n",
        "    add_eos: if to add \"\"<EOS> True: sentence length = max_len + 1\n",
        "                               False: sentence length = max_len\n",
        "    :return: [int, int, int ...]\n",
        "    '''\n",
        "\n",
        "    if add_eos:\n",
        "      assert(max_len != None)\n",
        "      max_len = max_len - 1\n",
        "\n",
        "    if max_len is not None:\n",
        "      if len(sentence) > max_len: # cut if sentence > max_len\n",
        "        sentence = sentence[:max_len]\n",
        "      else: # add padding if sentence < max_len\n",
        "        sentence = sentence + [self.PAD_TAG]*(max_len-len(sentence))\n",
        "      \n",
        "    if add_eos:\n",
        "      if sentence[-1] == self.PAD_TAG:  # if there is PAD in the sentence, add EOS before the TAG\n",
        "        pad_index = sentence.index(self.PAD_TAG)\n",
        "        sentence.insert(pad_index, self.EOS_TAG)\n",
        "      else: # No pad, and EOS at the end of the sentence\n",
        "        sentence.append(self.EOS_TAG)\n",
        "\n",
        "    result = [self.dict.get(i, self.UNK) for i in sentence]\n",
        "\n",
        "    return result\n",
        "  \n",
        "  def inverse_transform(self, indices):\n",
        "    '''vector 2 string\n",
        "    :param indices: [int, int, int, ...]\n",
        "    :return: \"123123...\"    \n",
        "    '''\n",
        "    result = []\n",
        "    for i in indices:\n",
        "      temp = self.inverse_dict.get(i, self.UNK_TAG)\n",
        "      if temp != self.EOS_TAG:  # delete everything after EOS\n",
        "        result.append(temp)\n",
        "      else:\n",
        "        break\n",
        "    return \"\".join(result)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dict)"
      ],
      "metadata": {
        "id": "EhZBqo7wqiQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sequence = Num_sequence()"
      ],
      "metadata": {
        "id": "qu9UyKX0urPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test num_Sequence\n",
        "print(num_sequence.dict)\n",
        "s = \"123123\"\n",
        "retVal = num_sequence.transform(s)\n",
        "print(retVal)\n",
        "retVal = num_sequence.inverse_transform(retVal)\n",
        "print(retVal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU8OTYTx4k-Z",
        "outputId": "9b6e9014-c44a-4e41-e26d-5193f4c57ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 1, '<UNK>': 0, '<SOS>': 2, '<EOS>': 3, '0': 4, '1': 5, '2': 6, '3': 7, '4': 8, '5': 9, '6': 10, '7': 11, '8': 12, '9': 13}\n",
            "[5, 6, 7, 5, 6, 7]\n",
            "123123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**  \n",
        "Prepare dataset and dataloader"
      ],
      "metadata": {
        "id": "EkMDv27gldpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. In targets of the samples, EOS and SOS are needed to label the start and the end of the network.  \n",
        "2. Add EOS in the target and transform.  "
      ],
      "metadata": {
        "id": "tl2BZgmCtfwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NumDataset(Dataset):\n",
        "  def __init__(self, train=True):\n",
        "    # generate random number with numpy\n",
        "    np.random.seed(10) if train else np.random.seed(11)\n",
        "    self.size = 400000 if train else 100000\n",
        "    self.data = np.random.randint(0, 1e8, size=[self.size])\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    input = list(str(self.data[index]))\n",
        "    target = input + ['0']\n",
        "    input_length = len(input)\n",
        "    target_length = len(target)\n",
        "    return input, target, input_length, target_length\n",
        "  \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "0IdOMtxrlfum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  '''\n",
        "  :param batch: [(input, label, input_length, label_length), (input, label, input_length, label_length)]\n",
        "  :return:\n",
        "  '''\n",
        "\n",
        "  batch = sorted(batch, key=lambda x: x[2], reverse=True) # big -> small\n",
        "  \n",
        "  input, target, input_length, target_length = zip(*batch)\n",
        "\n",
        "  input = [num_sequence.transform(i, max_len=max_len) for i in input]\n",
        "  target = [num_sequence.transform(i, max_len=max_len, add_eos=True) for i in target]\n",
        "  input = torch.LongTensor(input)\n",
        "  target = torch.LongTensor(target)\n",
        "\n",
        "  input_length = torch.LongTensor(input_length)\n",
        "  target_length = torch.LongTensor(target_length)\n",
        "\n",
        "\n",
        "  return input, target, input_length, target_length"
      ],
      "metadata": {
        "id": "i7aJ2Kdyoa5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_set = NumDataset(train=True)\n",
        "train_data_loader = DataLoader(data_set, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "ZiH0_HS4ns18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  print(input.size())\n",
        "  print(target.size())\n",
        "  print(input_length.shape)\n",
        "  print(target_length.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDgleUltC9nu",
        "outputId": "7ee037a1-2b8c-4b69-a80b-371e36ce65e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 10])\n",
            "torch.Size([256, 10])\n",
            "torch.Size([256])\n",
            "torch.Size([256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp = list(\"abcd\")\n",
        "temp += [\"ADD\"]\n",
        "print(temp)\n",
        "\n",
        "temp = list(\"abcd\")\n",
        "temp.append(\"ADD\")\n",
        "print(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uVqzRDDGCYX",
        "outputId": "3cd8d1e2-546a-4374-e529-e2aa9fcea9ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'ADD']\n",
            "['a', 'b', 'c', 'd', 'ADD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "t3o-xnVzlpZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using GRU, there are two API for accelerating the calculation.  \n",
        "1. pad_packed_sequence(out, batct_first, padding_value) *unpack*\n",
        "2. pack_padded_sequence(embedded, real_length, batch_first) *pack*\n",
        "3. Before using the two API, sort the batch in descending order."
      ],
      "metadata": {
        "id": "rlfiN7TQs5__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
      ],
      "metadata": {
        "id": "u-l8Wub8nFUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(num_sequence), embedding_dim=embedding_dim, padding_idx=num_sequence.PAD)\n",
        "    self.gru = nn.GRU(input_size=embedding_dim, num_layers=num_layers, hidden_size=hidden_size, batch_first=True, bidirectional=False, dropout=0)\n",
        "  \n",
        "  def forward(self, input, input_length):\n",
        "    '''\n",
        "    :param input: [batch_size, max_len]\n",
        "    :return \n",
        "    '''\n",
        "    embeded = self.embedding(input) # [batch_size, max_len, embedding_dim]\n",
        "\n",
        "    # pack to accelerate calculation\n",
        "    embeded = pack_padded_sequence(embeded, input_length.cpu(), batch_first=True)\n",
        "\n",
        "    output, hidden = self.gru(embeded)\n",
        "\n",
        "    # unpack\n",
        "    output, output_length = pad_packed_sequence(output, batch_first=True, padding_value=num_sequence.PAD)\n",
        "\n",
        "    # hidden: [1*1, batch_size, hidden_size]\n",
        "    # output: [batch_size, seq_len, hidden_size]\n",
        "    return output, hidden\n"
      ],
      "metadata": {
        "id": "YGWolPMclqo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "print(encoder)\n",
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  out, hidden = encoder(input, input_length)\n",
        "  print(out.size())\n",
        "  print(hidden.size())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSrN2eXfu0u0",
        "outputId": "5b234d45-05e8-490e-9d24-432c4dabb036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            ")\n",
            "torch.Size([256, 8, 64])\n",
            "torch.Size([1, 256, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder**"
      ],
      "metadata": {
        "id": "7wj9udQOnMWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The output of the encoder is a classification problem. We choose the output with a highest probability. \n",
        "2. The output of the decoder is [batch_size, max_len, vocab_size].\n",
        "3. Loss function: Cross Entropy"
      ],
      "metadata": {
        "id": "0zR0yKFooXS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "JNqldpOu1q8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=len(num_sequence), embedding_dim=embedding_dim,padding_idx=num_sequence.PAD)\n",
        "    # hidden_state = [1, batch_size, hidden_size]\n",
        "    self.gru = nn.GRU(input_size=embedding_dim,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=num_layers,\n",
        "                      batch_first=True,\n",
        "                      bidirectional=False,\n",
        "                      dropout=0)\n",
        "    # encoder_hidden_t: [2, batch_size, hidden_size]\n",
        "    self.fc = nn.Linear(hidden_size, len(num_sequence))\n",
        "  \n",
        "  def forward(self, target, encoder_hidden):\n",
        "    # 1. For the first time, decoder has the same hidden state as the encoder\n",
        "    decoder_hidden = encoder_hidden # [1, batch_size, hidden_size]\n",
        "    # 2. For the first time, input of decoder is the SOS with size of [batch_size, 1]\n",
        "    batch_size = encoder_hidden.size(1)\n",
        "    # decoder_input = torch.LongTensor(torch.ones([batch_size, 1], dtype=torch.int64)*num_sequence.SOS).to(device)\n",
        "    decoder_input = torch.LongTensor([[num_sequence.SOS]]*batch_size).to(device)\n",
        "    # 3. Calculate at the first time stamp, get output and hidden_state\n",
        "\n",
        "    # 4. Calculate the next output according to previous output\n",
        "    # 5. Put previous hidden_state and output as current hidden_state and input\n",
        "    # 6. Recurrsion step 4 and step 5\n",
        "\n",
        "    # Save the result of prediction\n",
        "    # [batch_size, max_len, vocab_size]\n",
        "    decoder_outputs = torch.zeros([batch_size, max_len, len(num_sequence)]).to(device)\n",
        "\n",
        "    for t in range(max_len):\n",
        "      decoder_output_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "      # decoder_output_t: [batch_size, vocab_size]; decoder_hidden: [1, batch_size, hidden_size]\n",
        "      # save decoder_output_t\n",
        "      decoder_outputs[:,t,:] = decoder_output_t\n",
        "\n",
        "      value, index = decoder_output_t.max(dim=-1)\n",
        "      decoder_input = index.unsqueeze(-1)\n",
        "    \n",
        "    return decoder_outputs, decoder_hidden\n",
        "  \n",
        "  def forward_step(self, decoder_input, decoder_hidden):\n",
        "    '''\n",
        "    calculate output at each time stamp\n",
        "    :param decoder_input: [batch_size, 1]\n",
        "    :param decoder_hidden: [1, batch_size, hidden_size]\n",
        "    :return:    \n",
        "    '''\n",
        "    decoder_input_embedded = self.embedding(decoder_input)  # [batch_size, 1, embedding_dim]\n",
        "\n",
        "    # out: [batch_size, 1, hidden_size] It is 1 because at the first point seq_len=1\n",
        "    # decoder_hidden: [1, batch_size, hidden_size]\n",
        "    out, decoder_hidden = self.gru(decoder_input_embedded, decoder_hidden)\n",
        "    # out: [batch_size, 1, hidden_size]\n",
        "    out = out.squeeze(dim=1) # [batch_size, hidden_size]\n",
        "    out = self.fc(out)  # [batch_size, vocab_size]\n",
        "    output = F.log_softmax(out, dim=-1) # [batch_size, vocab_size]\n",
        "\n",
        "    return output, decoder_hidden\n",
        "\n",
        "  \n",
        "  def evaluate(self, encoder_hidden):\n",
        "    # First time step, decoder_hidden is the encoder_hidden\n",
        "    decoder_hidden = encoder_hidden # [1,batch_size,encoder_hidden_size]\n",
        "    # First time step, input is the [batch_size, 1]\n",
        "    batch_size = encoder_hidden.size(1)\n",
        "    decoder_input = torch.LongTensor(torch.ones([batch_size, 1], dtype=torch.int64)*num_sequence.SOS).to(device)\n",
        "    \n",
        "    # Buffer to store the outputs\n",
        "    # [batch_size, max_len, vocab_size]\n",
        "    decoder_output = torch.zeros([batch_size, max_len, len(num_sequence)]).to(device)\n",
        "\n",
        "    decoder_predict = []  # [max_len, batch_size]\n",
        "\n",
        "    for t in range(max_len):\n",
        "      decoder_output_t, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
        "      decoder_output[:,t,:] = decoder_output_t\n",
        "\n",
        "      value, index = torch.max(decoder_output_t, 1)\n",
        "      decoder_input = index.unsqueeze(-1) # [batch_size, 1]\n",
        "      decoder_predict.append(index.cpu().detach().numpy())\n",
        "    \n",
        "    decoder_predict = np.array(decoder_predict).transpose() # [batch_size, max_len]\n",
        "    return decoder_outputs, decoder_predict\n",
        "    "
      ],
      "metadata": {
        "id": "tCV0XPxKnZy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()\n",
        "decoder = Decoder()\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "for input, target, input_length, target_length in train_data_loader:\n",
        "  out, encoder_hidden = encoder(input, input_length)\n",
        "  decoder(target, encoder_hidden)\n",
        "  print(out.size())\n",
        "  print(hidden.size())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X77HPtfrSGxc",
        "outputId": "467584a5-be7d-42ab-9e74-9e6b27cce369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(14, 100, padding_idx=1)\n",
            "  (gru): GRU(100, 64, batch_first=True)\n",
            "  (fc): Linear(in_features=64, out_features=14, bias=True)\n",
            ")\n",
            "torch.Size([256, 8, 64])\n",
            "torch.Size([1, 256, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine encoder and decoder to get seq2seq"
      ],
      "metadata": {
        "id": "wEJkYFM93lpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder()\n",
        "    self.decoder = Decoder()\n",
        "  \n",
        "  def forward(self, input, target, input_length, target_length):\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input, input_length)\n",
        "    decoder_outputs, decoder_hidden = self.decoder(target, encoder_hidden)\n",
        "    # print('encoder_outputs', encoder_outputs.shape)\n",
        "    # print('decoder_outputs', decoder_outputs.shape)\n",
        "    # print('target', target.size())\n",
        "    return decoder_outputs, decoder_hidden\n",
        "  \n",
        "  def evaluate(self, input, input_length):\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input, input_length)\n",
        "    decoder_outputs, decoder_predict = self.decoder.evaluate(encoder_hidden)\n",
        "    return decoder_outputs, decoder_predict\n",
        "    "
      ],
      "metadata": {
        "id": "j0Dd4aPL3roX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Initialize the model, optimizer and loss.  \n",
        "2. Traverse dataloader.\n",
        "3. Produce output from the model\n",
        "4. Calculate the loss\n",
        "5. Save and Load the model\n"
      ],
      "metadata": {
        "id": "JiVjaWUr4ttc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "HGkXuff-5HUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq = Seq2Seq().to(device)\n",
        "optimizer = Adam(seq2seq.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "TjIOXYkC5B5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Loop**"
      ],
      "metadata": {
        "id": "SZrzzIy54hdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "eP5tdOa5ICLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory\n",
        "if not os.path.exists(\"./models\"):\n",
        "  os.mkdir(\"./models\")"
      ],
      "metadata": {
        "id": "zmQfOErdIPiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for index, (input, target, input_length, target_length) in enumerate(train_data_loader):\n",
        "    input, target, input_length, target_length = input.to(device), target.to(device), input_length.to(device), target_length.to(device)\n",
        "    decoder_outputs, _ = seq2seq(input, target, input_length, target_length)\n",
        "    # print(decoder_outputs.size(), target.size())\n",
        "    decoder_outputs = decoder_outputs.view(-1, len(num_sequence)) # [batch_size*seq_len, -1]\n",
        "    # print(decoder_outputs.size(), target.size())\n",
        "    target = target.view(-1)  # [batch_size*seq_len]\n",
        "    loss = F.nll_loss(decoder_outputs, target)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Record the Training Phase\n",
        "    if index % 300 == 0:\n",
        "      torch.save(seq2seq.state_dict(), \"./models/model.pkl\")\n",
        "      torch.save(optimizer.state_dict(), \"./models/optimizer.pkl\")\n",
        "      pickle.dump(loss_list, open(\"./models/loss_list.pkl\", \"wb\"))\n",
        "      print(\"epoch: {}\\t idx:{} \\t loss:{: .6f}\".format(epoch, index, loss.item()))"
      ],
      "metadata": {
        "id": "4MwFa6rg3lAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7dc58a3-7188-413f-ce2b-34c75ff0cc1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\t idx:0 \t loss: 2.656001\n",
            "epoch: 0\t idx:300 \t loss: 1.269240\n",
            "epoch: 0\t idx:600 \t loss: 0.955149\n",
            "epoch: 0\t idx:900 \t loss: 0.736737\n",
            "epoch: 0\t idx:1200 \t loss: 0.551190\n",
            "epoch: 0\t idx:1500 \t loss: 0.379251\n",
            "epoch: 1\t idx:0 \t loss: 0.346296\n",
            "epoch: 1\t idx:300 \t loss: 0.260460\n",
            "epoch: 1\t idx:600 \t loss: 0.170203\n",
            "epoch: 1\t idx:900 \t loss: 0.124829\n",
            "epoch: 1\t idx:1200 \t loss: 0.087696\n",
            "epoch: 1\t idx:1500 \t loss: 0.070656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**"
      ],
      "metadata": {
        "id": "GYrH8St8hnNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_input = \"123456\""
      ],
      "metadata": {
        "id": "IYOvKyxzh1qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Seq2Seq().to(device)\n",
        "model.load_state_dict(torch.load(\"./models/model.pkl\"))\n",
        "input = list(str(_input))\n",
        "input_length = torch.LongTensor([len(input)]) # [1]\n",
        "input = torch.LongTensor([num_sequence.transform(input)])"
      ],
      "metadata": {
        "id": "PtJwOPlEhpPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  input = input.to(device)\n",
        "  input_length = input_length.to(device)\n",
        "  _, decoder_predict = model.evaluate(input, input_length)  # [batch_size, max_len, vocab_size]\n",
        "  pred = [num_sequence.inverse_transform(i) for i in decoder_predict]"
      ],
      "metadata": {
        "id": "aAlS8H7ziNB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(_input, \"---->\", pred[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jadm-VFti7QY",
        "outputId": "cf007258-c19e-4605-f792-bd448ad862d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123456 ----> 1234560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "lLoIt0esvSoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Data\n",
        "test_data_set = NumDataset(train=False)\n",
        "test_data_loader = DataLoader(test_data_set, batch_size=test_batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "OfGNYt1lsolh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Prediction\n",
        "test_model = Seq2Seq().to(device)\n",
        "test_model.load_state_dict(torch.load(\"./models/model.pkl\"))"
      ],
      "metadata": {
        "id": "Ac6MHGh6wr9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eede76d-156b-44f8-80c8-48234276dd9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inverse Transform\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "with torch.no_grad():\n",
        "  for idx, (input, target, input_len, target_len) in enumerate(test_data_loader):\n",
        "    input, target, input_len, target_len = input.to(device), target.to(device), input_length.to(device), target_length.to(device)\n",
        "    decoder_outputs, decoder_predict = test_model.evaluate(input, input_len) # [batch_size, max_len, vocab_size]\n",
        "    loss = F.nll_loss(decoder_outputs.view(-1, len(num_sequence)), target.view(-1), ignore_index = num_sequence.PAD)\n",
        "\n",
        "    target_inverse_transformed = [num_sequence.inverse_transform(i) for i in target.numpy()]\n",
        "    predict_inverse_transformed = [num_sequence.inverse_transform(i) for i in decoder_predict]\n",
        "    cur_eq = [1 if target_inverse_transformed[i] == predict_inverse_transformed[i] else 0 for i in range(len(target_inverse_transformed))]\n",
        "    acc_list.extend(cur_eq)"
      ],
      "metadata": {
        "id": "j9lE1rRqxBef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean acc: {} Mean loss:{:.6f}\".format(np.mean(acc_list), np.mean(loss_list)))"
      ],
      "metadata": {
        "id": "Cm959FTwznYB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}