{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03.CBOW.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Continuous Bag of Word**"
      ],
      "metadata": {
        "id": "1L0D0ou41q0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ymowwiKz1BzW"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, x, y, v2i, i2v):\n",
        "        self.x, self.y = x, y\n",
        "        self.v2i, self.i2v = v2i, i2v\n",
        "        self.vocab = v2i.keys()\n",
        "\n",
        "    def sample(self, n):\n",
        "        b_idx = np.random.randint(0, len(self.x), n)\n",
        "        bx, by = self.x[b_idx], self.y[b_idx]\n",
        "        return bx, by\n",
        "\n",
        "    @property\n",
        "    def num_word(self):\n",
        "        return len(self.v2i)"
      ],
      "metadata": {
        "id": "iJfjROjX8Z-v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_w2v_data(corpus, skip_window=2, method=\"skip_gram\"):\n",
        "    all_words = [sentence.split(\" \") for sentence in corpus]\n",
        "    all_words = np.array(list(itertools.chain(*all_words)))\n",
        "    # vocab sort by decreasing frequency for the negative sampling below (nce_loss).\n",
        "    vocab, v_count = np.unique(all_words, return_counts=True)\n",
        "    vocab = vocab[np.argsort(v_count)[::-1]]\n",
        "\n",
        "    print(\"all vocabularies sorted from more frequent to less frequent:\\n\", vocab)\n",
        "    v2i = {v: i for i, v in enumerate(vocab)}\n",
        "    i2v = {i: v for v, i in v2i.items()}\n",
        "\n",
        "    # pair data\n",
        "    pairs = []\n",
        "    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]\n",
        "\n",
        "    for c in corpus:\n",
        "        words = c.split(\" \")\n",
        "        w_idx = [v2i[w] for w in words]\n",
        "        if method == \"skip_gram\":\n",
        "            for i in range(len(w_idx)):\n",
        "                for j in js:\n",
        "                    if i + j < 0 or i + j >= len(w_idx):\n",
        "                        continue\n",
        "                    pairs.append((w_idx[i], w_idx[i + j]))  # (center, context) or (feature, target)\n",
        "        elif method.lower() == \"cbow\":\n",
        "            for i in range(skip_window, len(w_idx) - skip_window):\n",
        "                context = []\n",
        "                for j in js:\n",
        "                    context.append(w_idx[i + j])\n",
        "                pairs.append(context + [w_idx[i]])  # (contexts, center) or (feature, target)\n",
        "        else:\n",
        "            raise ValueError\n",
        "    pairs = np.array(pairs)\n",
        "    print(\"5 example pairs:\\n\", pairs[:5])\n",
        "    if method.lower() == \"skip_gram\":\n",
        "        x, y = pairs[:, 0], pairs[:, 1]\n",
        "    elif method.lower() == \"cbow\":\n",
        "        x, y = pairs[:, :-1], pairs[:, -1]\n",
        "    else:\n",
        "        raise ValueError\n",
        "    return Dataset(x, y, v2i, i2v)"
      ],
      "metadata": {
        "id": "btvdF1tW2yjx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fake Data**"
      ],
      "metadata": {
        "id": "saATPkeF2_0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    # numbers\n",
        "    \"5 2 4 8 6 2 3 6 4\",\n",
        "    \"4 8 5 6 9 5 5 6\",\n",
        "    \"1 1 5 2 3 3 8\",\n",
        "    \"3 6 9 6 8 7 4 6 3\",\n",
        "    \"8 9 9 6 1 4 3 4\",\n",
        "    \"1 0 2 0 2 1 3 3 3 3 3\",\n",
        "    \"9 3 3 0 1 4 7 8\",\n",
        "    \"9 9 8 5 6 7 1 2 3 0 1 0\",\n",
        "\n",
        "    # alphabets, expecting that 9 is close to letters\n",
        "    \"a t g q e h 9 u f\",\n",
        "    \"e q y u o i p s\",\n",
        "    \"q o 9 p l k j o k k o p\",\n",
        "    \"h g y i u t t a e q\",\n",
        "    \"i k d q r e 9 e a d\",\n",
        "    \"o p d g 9 s a f g a\",\n",
        "    \"i u y g h k l a s w\",\n",
        "    \"o l u y a o g f s\",\n",
        "    \"o p i u y g d a s j d l\",\n",
        "    \"u k i l o 9 l j s\",\n",
        "    \"y g i s h k j l f r f\",\n",
        "    \"i o h n 9 9 d 9 f a 9\",\n",
        "]"
      ],
      "metadata": {
        "id": "62NP4WHe3BTy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word to Vector**"
      ],
      "metadata": {
        "id": "fhPPpuNl547O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "fmiR_8IF8NbD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_w2v_data(corpus, skip_window=2, method=\"skip_gram\"):\n",
        "    all_words = [sentence.split(\" \") for sentence in corpus]\n",
        "    all_words = np.array(list(itertools.chain(*all_words)))\n",
        "    # vocab sort by decreasing frequency for the negative sampling below (nce_loss).\n",
        "    vocab, v_count = np.unique(all_words, return_counts=True)\n",
        "    vocab = vocab[np.argsort(v_count)[::-1]]\n",
        "\n",
        "    print(\"all vocabularies sorted from more frequent to less frequent:\\n\", vocab)\n",
        "    v2i = {v: i for i, v in enumerate(vocab)}\n",
        "    i2v = {i: v for v, i in v2i.items()}\n",
        "\n",
        "    # pair data\n",
        "    pairs = []\n",
        "    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]\n",
        "\n",
        "    for c in corpus:\n",
        "        words = c.split(\" \")\n",
        "        w_idx = [v2i[w] for w in words]\n",
        "        if method == \"skip_gram\":\n",
        "            for i in range(len(w_idx)):\n",
        "                for j in js:\n",
        "                    if i + j < 0 or i + j >= len(w_idx):\n",
        "                        continue\n",
        "                    pairs.append((w_idx[i], w_idx[i + j]))  # (center, context) or (feature, target)\n",
        "        elif method.lower() == \"cbow\":\n",
        "            for i in range(skip_window, len(w_idx) - skip_window):\n",
        "                context = []\n",
        "                for j in js:\n",
        "                    context.append(w_idx[i + j])\n",
        "                pairs.append(context + [w_idx[i]])  # (contexts, center) or (feature, target)\n",
        "        else:\n",
        "            raise ValueError\n",
        "    pairs = np.array(pairs)\n",
        "    print(\"5 example pairs:\\n\", pairs[:5])\n",
        "    if method.lower() == \"skip_gram\":\n",
        "        x, y = pairs[:, 0], pairs[:, 1]\n",
        "    elif method.lower() == \"cbow\":\n",
        "        x, y = pairs[:, :-1], pairs[:, -1]\n",
        "    else:\n",
        "        raise ValueError\n",
        "    return Dataset(x, y, v2i, i2v)"
      ],
      "metadata": {
        "id": "B8sg8B_o4W85"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "Xtb_GhZY3ZX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(keras.Model):\n",
        "    def __init__(self, v_dim, emb_dim):\n",
        "        super().__init__()\n",
        "        self.v_dim = v_dim\n",
        "        self.embeddings = keras.layers.Embedding(\n",
        "            input_dim=v_dim, output_dim=emb_dim,  # [n_vocab, emb_dim]\n",
        "            embeddings_initializer=keras.initializers.RandomNormal(0., 0.1),\n",
        "        )\n",
        "\n",
        "        # noise-contrastive estimation\n",
        "        self.nce_w = self.add_weight(\n",
        "            name=\"nce_w\", shape=[v_dim, emb_dim],\n",
        "            initializer=keras.initializers.TruncatedNormal(0., 0.1))  # [n_vocab, emb_dim]\n",
        "        self.nce_b = self.add_weight(\n",
        "            name=\"nce_b\", shape=(v_dim,),\n",
        "            initializer=keras.initializers.Constant(0.1))  # [n_vocab, ]\n",
        "\n",
        "        self.opt = keras.optimizers.Adam(0.01)\n",
        "\n",
        "    def call(self, x, training=None, mask=None):\n",
        "        # x.shape = [n, skip_window*2]\n",
        "        o = self.embeddings(x)          # [n, skip_window*2, emb_dim]\n",
        "        o = tf.reduce_mean(o, axis=1)   # [n, emb_dim]\n",
        "        return o\n",
        "\n",
        "    # negative sampling: take one positive label and num_sampled negative labels to compute the loss\n",
        "    # in order to reduce the computation of full softmax\n",
        "    def loss(self, x, y, training=None):\n",
        "        embedded = self.call(x, training)\n",
        "        return tf.reduce_mean(\n",
        "            tf.nn.nce_loss(\n",
        "                weights=self.nce_w, biases=self.nce_b, labels=tf.expand_dims(y, axis=1),\n",
        "                inputs=embedded, num_sampled=5, num_classes=self.v_dim))\n",
        "\n",
        "    def step(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss(x, y, True)\n",
        "            grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return loss.numpy()\n",
        "\n",
        "\n",
        "def train(model, data):\n",
        "    for t in range(2500):\n",
        "        bx, by = data.sample(8)\n",
        "        loss = model.step(bx, by)\n",
        "        if t % 200 == 0:\n",
        "            print(\"step: {} | loss: {}\".format(t, loss))"
      ],
      "metadata": {
        "id": "MNFMVu3a3atY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Graph Tool**"
      ],
      "metadata": {
        "id": "aJLNrmvb7sU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "LyyMekIc72p0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_w2v_word_embedding(model, data, path=None):\n",
        "    word_emb = model.embeddings.get_weights()[0]\n",
        "    for i in range(data.num_word):\n",
        "        c = \"blue\"\n",
        "        try:\n",
        "            int(data.i2v[i])\n",
        "        except ValueError:\n",
        "            c = \"red\"\n",
        "        plt.text(word_emb[i, 0], word_emb[i, 1], s=data.i2v[i], color=c, weight=\"bold\")\n",
        "    plt.xlim(word_emb[:, 0].min() - .5, word_emb[:, 0].max() + .5)\n",
        "    plt.ylim(word_emb[:, 1].min() - .5, word_emb[:, 1].max() + .5)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.xlabel(\"embedding dim1\")\n",
        "    plt.ylabel(\"embedding dim2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jGbZWb-h7vzF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process Words**"
      ],
      "metadata": {
        "id": "6u9yLqPV8D2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = process_w2v_data(corpus, skip_window=2, method=\"cbow\") # D is the Dataset object"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQcXagyx8FXU",
        "outputId": "e63b3667-fa61-44e5-f2c6-8d557316658a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all vocabularies sorted from more frequent to less frequent:\n",
            " ['9' '3' 'o' '6' 'a' '1' 'i' 'g' 's' '4' 'l' 'k' '8' 'u' '2' 'd' '5' 'y'\n",
            " 'f' 'e' 'h' 'p' 'q' '0' 'j' '7' 't' 'r' 'w' 'n']\n",
            "5 example pairs:\n",
            " [[16 14 12  3  9]\n",
            " [14  9  3 14 12]\n",
            " [ 9 12 14  1  3]\n",
            " [12  3  1  3 14]\n",
            " [ 3 14  3  9  1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = CBOW(v_dim=d.num_word, emb_dim=2)"
      ],
      "metadata": {
        "id": "HRlVlPBj8rL_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model=m, data=d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAtAm5e780us",
        "outputId": "c51e8733-54af-4565-d515-f3aeacccd00c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 | loss: 10.43246078491211\n",
            "step: 200 | loss: 4.005064487457275\n",
            "step: 400 | loss: 2.4908549785614014\n",
            "step: 600 | loss: 2.5087735652923584\n",
            "step: 800 | loss: 2.1871142387390137\n",
            "step: 1000 | loss: 2.1817879676818848\n",
            "step: 1200 | loss: 2.5733275413513184\n",
            "step: 1400 | loss: 2.6166064739227295\n",
            "step: 1600 | loss: 2.6806259155273438\n",
            "step: 1800 | loss: 2.140188217163086\n",
            "step: 2000 | loss: 2.1616530418395996\n",
            "step: 2200 | loss: 2.819303512573242\n",
            "step: 2400 | loss: 2.1683509349823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting**"
      ],
      "metadata": {
        "id": "kzJQ2JF19ZUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_w2v_word_embedding(m, d)"
      ],
      "metadata": {
        "id": "wxJZnmTP9bZi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "9e120e55-a8fb-41c9-f6a0-1d50234f2d0c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}