{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04.Skip-Gram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Skip-Gram**"
      ],
      "metadata": {
        "id": "u26C-Z4_gaqV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "imUVsxhvfCo5"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, x, y, v2i, i2v):\n",
        "        self.x, self.y = x, y\n",
        "        self.v2i, self.i2v = v2i, i2v\n",
        "        self.vocab = v2i.keys()\n",
        "\n",
        "    def sample(self, n):\n",
        "        b_idx = np.random.randint(0, len(self.x), n)\n",
        "        bx, by = self.x[b_idx], self.y[b_idx]\n",
        "        return bx, by\n",
        "\n",
        "    @property\n",
        "    def num_word(self):\n",
        "        return len(self.v2i)"
      ],
      "metadata": {
        "id": "lPVn-dzkgfJU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_w2v_data(corpus, skip_window=2, method=\"skip_gram\"):\n",
        "    all_words = [sentence.split(\" \") for sentence in corpus]\n",
        "    all_words = np.array(list(itertools.chain(*all_words)))\n",
        "    # vocab sort by decreasing frequency for the negative sampling below (nce_loss).\n",
        "    vocab, v_count = np.unique(all_words, return_counts=True)\n",
        "    vocab = vocab[np.argsort(v_count)[::-1]]\n",
        "\n",
        "    print(\"all vocabularies sorted from more frequent to less frequent:\\n\", vocab)\n",
        "    v2i = {v: i for i, v in enumerate(vocab)}\n",
        "    i2v = {i: v for v, i in v2i.items()}\n",
        "\n",
        "    # pair data\n",
        "    pairs = []\n",
        "    js = [i for i in range(-skip_window, skip_window + 1) if i != 0]\n",
        "\n",
        "    for c in corpus:\n",
        "        words = c.split(\" \")\n",
        "        w_idx = [v2i[w] for w in words]\n",
        "        if method == \"skip_gram\":\n",
        "            for i in range(len(w_idx)):\n",
        "                for j in js:\n",
        "                    if i + j < 0 or i + j >= len(w_idx):\n",
        "                        continue\n",
        "                    pairs.append((w_idx[i], w_idx[i + j]))  # (center, context) or (feature, target)\n",
        "        elif method.lower() == \"cbow\":\n",
        "            for i in range(skip_window, len(w_idx) - skip_window):\n",
        "                context = []\n",
        "                for j in js:\n",
        "                    context.append(w_idx[i + j])\n",
        "                pairs.append(context + [w_idx[i]])  # (contexts, center) or (feature, target)\n",
        "        else:\n",
        "            raise ValueError\n",
        "    pairs = np.array(pairs)\n",
        "    print(\"5 example pairs:\\n\", pairs[:5])\n",
        "    if method.lower() == \"skip_gram\":\n",
        "        x, y = pairs[:, 0], pairs[:, 1]\n",
        "    elif method.lower() == \"cbow\":\n",
        "        x, y = pairs[:, :-1], pairs[:, -1]\n",
        "    else:\n",
        "        raise ValueError\n",
        "    return Dataset(x, y, v2i, i2v)"
      ],
      "metadata": {
        "id": "ptJHc1ejgwcs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    # numbers\n",
        "    \"5 2 4 8 6 2 3 6 4\",\n",
        "    \"4 8 5 6 9 5 5 6\",\n",
        "    \"1 1 5 2 3 3 8\",\n",
        "    \"3 6 9 6 8 7 4 6 3\",\n",
        "    \"8 9 9 6 1 4 3 4\",\n",
        "    \"1 0 2 0 2 1 3 3 3 3 3\",\n",
        "    \"9 3 3 0 1 4 7 8\",\n",
        "    \"9 9 8 5 6 7 1 2 3 0 1 0\",\n",
        "\n",
        "    # alphabets, expecting that 9 is close to letters\n",
        "    \"a t g q e h 9 u f\",\n",
        "    \"e q y u o i p s\",\n",
        "    \"q o 9 p l k j o k k o p\",\n",
        "    \"h g y i u t t a e q\",\n",
        "    \"i k d q r e 9 e a d\",\n",
        "    \"o p d g 9 s a f g a\",\n",
        "    \"i u y g h k l a s w\",\n",
        "    \"o l u y a o g f s\",\n",
        "    \"o p i u y g d a s j d l\",\n",
        "    \"u k i l o 9 l j s\",\n",
        "    \"y g i s h k j l f r f\",\n",
        "    \"i o h n 9 9 d 9 f a 9\",\n",
        "]"
      ],
      "metadata": {
        "id": "vuqeAqxGg26k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip-Gram Model**"
      ],
      "metadata": {
        "id": "3iv7aV0ShMGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipGram(keras.Model):\n",
        "    def __init__(self, v_dim, emb_dim):\n",
        "        super().__init__()\n",
        "        self.v_dim = v_dim\n",
        "        self.embeddings = keras.layers.Embedding(\n",
        "            input_dim=v_dim, output_dim=emb_dim,       # [n_vocab, emb_dim]\n",
        "            embeddings_initializer=keras.initializers.RandomNormal(0., 0.1),\n",
        "        )\n",
        "\n",
        "        # noise-contrastive estimation\n",
        "        self.nce_w = self.add_weight(\n",
        "            name=\"nce_w\", shape=[v_dim, emb_dim],\n",
        "            initializer=keras.initializers.TruncatedNormal(0., 0.1))  # [n_vocab, emb_dim]\n",
        "        self.nce_b = self.add_weight(\n",
        "            name=\"nce_b\", shape=(v_dim,),\n",
        "            initializer=keras.initializers.Constant(0.1))  # [n_vocab, ]\n",
        "\n",
        "        self.opt = keras.optimizers.Adam(0.01)\n",
        "\n",
        "    def call(self, x, training=None, mask=None):\n",
        "        # x.shape = [n, ]\n",
        "        o = self.embeddings(x)      # [n, emb_dim]\n",
        "        return o\n",
        "\n",
        "    # negative sampling: take one positive label and num_sampled negative labels to compute the loss\n",
        "    # in order to reduce the computation of full softmax\n",
        "    def loss(self, x, y, training=None):\n",
        "        embedded = self.call(x, training)\n",
        "        return tf.reduce_mean(\n",
        "            tf.nn.nce_loss(\n",
        "                weights=self.nce_w, biases=self.nce_b, labels=tf.expand_dims(y, axis=1),\n",
        "                inputs=embedded, num_sampled=5, num_classes=self.v_dim))\n",
        "\n",
        "    def step(self, x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.loss(x, y, True)\n",
        "            grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return loss.numpy()"
      ],
      "metadata": {
        "id": "2sc3U9cHhLLm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data):\n",
        "    for t in range(2500):\n",
        "        bx, by = data.sample(8)\n",
        "        loss = model.step(bx, by)\n",
        "        if t % 200 == 0:\n",
        "            print(\"step: {} | loss: {}\".format(t, loss))"
      ],
      "metadata": {
        "id": "30aXyCnBhl1n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process**"
      ],
      "metadata": {
        "id": "gJNO_54-hnIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = process_w2v_data(corpus, skip_window=2, method=\"skip_gram\")\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nLYe3S_hojE",
        "outputId": "e81c6f85-1c09-48fa-cf63-43080d56d97f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all vocabularies sorted from more frequent to less frequent:\n",
            " ['9' '3' 'o' '6' 'a' '1' 'i' 'g' 's' '4' 'l' 'k' '8' 'u' '2' 'd' '5' 'y'\n",
            " 'f' 'e' 'h' 'p' 'q' '0' 'j' '7' 't' 'r' 'w' 'n']\n",
            "5 example pairs:\n",
            " [[16 14]\n",
            " [16  9]\n",
            " [14 16]\n",
            " [14  9]\n",
            " [14 12]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Dataset at 0x7f61da3d2950>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = SkipGram(d.num_word, 2)"
      ],
      "metadata": {
        "id": "mDBNKgI0h16j"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model=m,data=d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzBOwC4Tisxj",
        "outputId": "2f553a16-497f-4c2f-f082-7dbeca06f888"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 | loss: 5.946027755737305\n",
            "step: 200 | loss: 2.8823986053466797\n",
            "step: 400 | loss: 2.6079299449920654\n",
            "step: 600 | loss: 2.9627768993377686\n",
            "step: 800 | loss: 2.55483341217041\n",
            "step: 1000 | loss: 2.5986528396606445\n",
            "step: 1200 | loss: 2.3097195625305176\n",
            "step: 1400 | loss: 2.810511589050293\n",
            "step: 1600 | loss: 2.3073134422302246\n",
            "step: 1800 | loss: 2.386871814727783\n",
            "step: 2000 | loss: 2.0080699920654297\n",
            "step: 2200 | loss: 1.7148692607879639\n",
            "step: 2400 | loss: 1.8562114238739014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting**"
      ],
      "metadata": {
        "id": "E3LSAallinAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XgpmGmmAi_vq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_w2v_word_embedding(model, data, path=None):\n",
        "    word_emb = model.embeddings.get_weights()[0]\n",
        "    for i in range(data.num_word):\n",
        "        c = \"blue\"\n",
        "        try:\n",
        "            int(data.i2v[i])\n",
        "        except ValueError:\n",
        "            c = \"red\"\n",
        "        plt.text(word_emb[i, 0], word_emb[i, 1], s=data.i2v[i], color=c, weight=\"bold\")\n",
        "    plt.xlim(word_emb[:, 0].min() - .5, word_emb[:, 0].max() + .5)\n",
        "    plt.ylim(word_emb[:, 1].min() - .5, word_emb[:, 1].max() + .5)\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "    plt.xlabel(\"embedding dim1\")\n",
        "    plt.ylabel(\"embedding dim2\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hewmiKSIjBRy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_w2v_word_embedding(model=m, data=d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "jCrvVAKojD4M",
        "outputId": "643afbbf-2316-4a87-fc9f-afbb2d22e23a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}